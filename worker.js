
import re
import time
import asyncio
import base64
import yaml
import json
import io

import uuid
import random
import shutil
import html
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, unquote, parse_qsl, urlencode, urlunparse
from typing import Optional

from telegram import Update, BotCommand, InputFile, InlineKeyboardButton, InlineKeyboardMarkup, Message, Bot
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler

from telegram.error import BadRequest, TimedOut, NetworkError

# ruamel.yaml å®¹é”™è§£æ
try:
    from ruamel.yaml import YAML

    _HAS_RUAMEL = True
    yaml_ruamel = YAML(typ="safe")
except ImportError:
    _HAS_RUAMEL = False

# --- å…¨å±€é…ç½®å¸¸é‡ ---
CONCURRENT_LIMIT = 5000
TEXT_MESSAGE_URL_LIMIT = 500
FILE_OUTPUT_THRESHOLD = 5
QUERY_URL_LIMIT = 5000
NODE_DISPLAY_LIMIT = 30
CACHE_TTL_SECONDS = 30 * 60  # å†…å­˜ç¼“å­˜è¿‡æœŸæ—¶é—´: 30åˆ†é’Ÿ

# --- å…¨å±€é…ç½® ---
CLIENT_USER_AGENTS = [
    "clash-verge-rev/2.3.2",
]

try:
    admin_id_str = '123456789'
    ADMIN_IDS = [int(admin_id.strip()) for admin_id in admin_id_str.split(',')]
    print(f"æˆåŠŸåŠ è½½ç®¡ç†å‘˜ID: {ADMIN_ID}")
except ValueError:
    print("é”™è¯¯ï¼šADMIN_ID ç¯å¢ƒå˜é‡æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿æ˜¯ä½¿ç”¨é€—å·åˆ†éš”çš„æ•°å­—ã€‚")
    ADMIN_IDS = []

# --- æ–‡ä»¶ä¸ç›®å½•å®šä¹‰ ---
CACHE_FILE = "valid_subs.txt"
INVALID_CACHE_FILE = "invalid_subs.txt"
VALID_NODES_FILE = "valid_nodes.txt"
DOMAIN_MAP_FILE = "domain_map.txt"
TEMP_YAML_DIR = "temp_yaml_files"

# --- æ­£åˆ™è¡¨è¾¾å¼ ---
RE_FILENAME_UTF8 = re.compile(r"filename\*=UTF-8''(.+)")
RE_FILENAME_SIMPLE = re.compile(r'filename="?([^";]+)"?')
RE_URLS = re.compile(r"https?://[^\s'\"#]+")
RE_NODE_LINKS = re.compile(
    r"^(vmess|ssr|ss|shadowsocks|scocks5|http|https|http2|wireguard|ssh|trojan|snell|hysteria|hysteria2|tuic|anytls|vless)://",
    re.IGNORECASE
)
RE_DIRECT_NODE_LINKS = re.compile(
    r"^(vmess|ssr|ss|shadowsocks|scocks5|wireguard|ssh|trojan|snell|hysteria|hysteria2|tuic|anytls|vless)://",
    re.IGNORECASE
)
RE_USERINFO_KV = re.compile(r"(\w+)=(\d+)")

# --- åœ°åŒºå…³é”®è¯ ---
COUNTRY_KEYWORDS = {
    'ä¸­å›½': ['CHINA', 'CHINANET', 'CHINA-NET', 'ä¸­å›½', 'ğŸ‡¨ğŸ‡³', 'CN-', 'CN_', '[CN]', 'ä¸­å›½èŠ‚ç‚¹', 'CNèŠ‚ç‚¹', 'æ•™è‚²ç½‘',
            'é•¿åŸ', 'è”é€š', 'ç”µä¿¡', 'ç§»åŠ¨', 'å¹¿ç”µ', 'åŒ—äº¬', 'ä¸Šæµ·', 'å¤©æ´¥', 'é‡åº†', 'æ²³åŒ—', 'çŸ³å®¶åº„', 'å”å±±', 'ç§¦çš‡å²›',
            'é‚¯éƒ¸', 'é‚¢å°', 'ä¿å®š', 'å¼ å®¶å£', 'æ‰¿å¾·', 'æ²§å·', 'å»ŠåŠ', 'è¡¡æ°´', 'å±±è¥¿', 'å¤ªåŸ', 'å¤§åŒ', 'é˜³æ³‰', 'é•¿æ²»',
            'æ™‹åŸ', 'æœ”å·', 'æ™‹ä¸­', 'è¿åŸ', 'å¿»å·', 'ä¸´æ±¾', 'å•æ¢', 'å†…è’™å¤', 'å‘¼å’Œæµ©ç‰¹', 'åŒ…å¤´', 'ä¹Œæµ·', 'èµ¤å³°',
            'é€šè¾½', 'é„‚å°”å¤šæ–¯', 'å‘¼ä¼¦è´å°”', 'å·´å½¦æ·–å°”', 'ä¹Œå…°å¯Ÿå¸ƒ', 'å…´å®‰ç›Ÿ', 'é”¡æ—éƒ­å‹’ç›Ÿ', 'é˜¿æ‹‰å–„ç›Ÿ', 'è¾½å®', 'æ²ˆé˜³',
            'å¤§è¿', 'éå±±', 'æŠšé¡º', 'æœ¬æºª', 'ä¸¹ä¸œ', 'é”¦å·', 'è¥å£', 'é˜œæ–°', 'è¾½é˜³', 'ç›˜é”¦', 'é“å²­', 'æœé˜³', 'è‘«èŠ¦å²›',
            'å‰æ—', 'é•¿æ˜¥', 'å‰æ—å¸‚', 'å››å¹³', 'è¾½æº', 'é€šåŒ–', 'ç™½å±±', 'æ¾åŸ', 'ç™½åŸ', 'å»¶è¾¹', 'é»‘é¾™æ±Ÿ', 'å“ˆå°”æ»¨',
            'é½é½å“ˆå°”', 'é¸¡è¥¿', 'é¹¤å²—', 'åŒé¸­å±±', 'å¤§åº†', 'ä¼Šæ˜¥', 'ä½³æœ¨æ–¯', 'ä¸ƒå°æ²³', 'ç‰¡ä¸¹æ±Ÿ', 'é»‘æ²³', 'ç»¥åŒ–',
            'å¤§å…´å®‰å²­', 'æ±Ÿè‹', 'å—äº¬', 'æ— é”¡', 'å¾å·', 'å¸¸å·', 'è‹å·', 'å—é€š', 'è¿äº‘æ¸¯', 'æ·®å®‰', 'ç›åŸ', 'æ‰¬å·',
            'é•‡æ±Ÿ', 'æ³°å·', 'å®¿è¿', 'æµ™æ±Ÿ', 'æ­å·', 'å®æ³¢', 'æ¸©å·', 'å˜‰å…´', 'æ¹–å·', 'ç»å…´', 'é‡‘å', 'è¡¢å·', 'èˆŸå±±',
            'å°å·', 'ä¸½æ°´', 'å®‰å¾½', 'åˆè‚¥', 'èŠœæ¹–', 'èšŒåŸ ', 'æ·®å—', 'é©¬éå±±', 'æ·®åŒ—', 'é“œé™µ', 'å®‰åº†', 'é»„å±±', 'æ»å·',
            'é˜œé˜³', 'å®¿å·', 'å…­å®‰', 'äº³å·', 'æ± å·', 'å®£åŸ', 'ç¦å»º', 'ç¦å·', 'å¦é—¨', 'è†ç”°', 'ä¸‰æ˜', 'æ³‰å·', 'æ¼³å·',
            'å—å¹³', 'é¾™å²©', 'å®å¾·', 'æ±Ÿè¥¿', 'å—æ˜Œ', 'æ™¯å¾·é•‡', 'èä¹¡', 'ä¹æ±Ÿ', 'æ–°ä½™', 'é¹°æ½­', 'èµ£å·', 'å‰å®‰', 'å®œæ˜¥',
            'æŠšå·', 'ä¸Šé¥¶', 'å±±ä¸œ', 'æµå—', 'é’å²›', 'æ·„åš', 'æ£åº„', 'ä¸œè¥', 'çƒŸå°', 'æ½åŠ', 'æµå®', 'æ³°å®‰', 'å¨æµ·',
            'æ—¥ç…§', 'ä¸´æ²‚', 'å¾·å·', 'èŠåŸ', 'æ»¨å·', 'èæ³½', 'æ²³å—', 'éƒ‘å·', 'å¼€å°', 'æ´›é˜³', 'å¹³é¡¶å±±', 'å®‰é˜³', 'é¹¤å£',
            'æ–°ä¹¡', 'ç„¦ä½œ', 'æ¿®é˜³', 'è®¸æ˜Œ', 'æ¼¯æ²³', 'ä¸‰é—¨å³¡', 'å—é˜³', 'å•†ä¸˜', 'ä¿¡é˜³', 'å‘¨å£', 'é©»é©¬åº—', 'æµæº', 'æ¹–åŒ—',
            'æ­¦æ±‰', 'é»„çŸ³', 'åå °', 'å®œæ˜Œ', 'è¥„é˜³', 'é„‚å·', 'è†é—¨', 'å­æ„Ÿ', 'è†å·', 'é»„å†ˆ', 'å’¸å®', 'éšå·', 'æ©æ–½',
            'æ¹–å—', 'é•¿æ²™', 'æ ªæ´²', 'æ¹˜æ½­', 'è¡¡é˜³', 'é‚µé˜³', 'å²³é˜³', 'å¸¸å¾·', 'å¼ å®¶ç•Œ', 'ç›Šé˜³', 'éƒ´å·', 'æ°¸å·', 'æ€€åŒ–',
            'å¨„åº•', 'æ¹˜è¥¿', 'å¹¿ä¸œ', 'å¹¿å·', 'æ·±åœ³', 'ç æµ·', 'æ±•å¤´', 'ä½›å±±', 'éŸ¶å…³', 'æ¹›æ±Ÿ', 'è‚‡åº†', 'æ±Ÿé—¨', 'èŒ‚å',
            'æƒ å·', 'æ¢…å·', 'æ±•å°¾', 'æ²³æº', 'é˜³æ±Ÿ', 'æ¸…è¿œ', 'ä¸œè', 'ä¸­å±±', 'æ½®å·', 'æ­é˜³', 'äº‘æµ®', 'å¹¿è¥¿', 'å—å®',
            'æŸ³å·', 'æ¡‚æ—', 'æ¢§å·', 'åŒ—æµ·', 'é˜²åŸæ¸¯', 'é’¦å·', 'è´µæ¸¯', 'ç‰æ—', 'ç™¾è‰²', 'è´ºå·', 'æ²³æ± ', 'æ¥å®¾', 'å´‡å·¦',
            'æµ·å—', 'æµ·å£', 'ä¸‰äºš', 'ä¸‰æ²™', 'å„‹å·', 'é‡åº†', 'å››å·', 'æˆéƒ½', 'è‡ªè´¡', 'æ”€æèŠ±', 'æ³¸å·', 'å¾·é˜³', 'ç»µé˜³',
            'å¹¿å…ƒ', 'é‚å®', 'å†…æ±Ÿ', 'ä¹å±±', 'å—å……', 'çœ‰å±±', 'å®œå®¾', 'å¹¿å®‰', 'è¾¾å·', 'é›…å®‰', 'å·´ä¸­', 'èµ„é˜³', 'é˜¿å',
            'ç”˜å­œ', 'å‡‰å±±', 'è´µå·', 'è´µé˜³', 'å…­ç›˜æ°´', 'éµä¹‰', 'å®‰é¡º', 'æ¯•èŠ‚', 'é“œä»', 'é»”è¥¿å—', 'é»”ä¸œå—', 'é»”å—', 'äº‘å—',
            'æ˜†æ˜', 'æ›²é–', 'ç‰æºª', 'ä¿å±±', 'æ˜­é€š', 'ä¸½æ±Ÿ', 'æ™®æ´±', 'ä¸´æ²§', 'æ¥šé›„', 'çº¢æ²³', 'æ–‡å±±', 'è¥¿åŒç‰ˆçº³', 'å¤§ç†',
            'å¾·å®', 'æ€’æ±Ÿ', 'è¿ªåº†', 'è¥¿è—', 'æ‹‰è¨', 'æ˜Œéƒ½', 'æ—èŠ', 'å±±å—', 'æ—¥å–€åˆ™', 'é‚£æ›²', 'é˜¿é‡Œ', 'é™•è¥¿', 'è¥¿å®‰',
            'é“œå·', 'å®é¸¡', 'å’¸é˜³', 'æ¸­å—', 'å»¶å®‰', 'æ±‰ä¸­', 'æ¦†æ—', 'å®‰åº·', 'å•†æ´›', 'ç”˜è‚ƒ', 'å…°å·', 'å˜‰å³ªå…³', 'é‡‘æ˜Œ',
            'ç™½é“¶', 'å¤©æ°´', 'æ­¦å¨', 'å¼ æ–', 'å¹³å‡‰', 'é…’æ³‰', 'åº†é˜³', 'å®šè¥¿', 'é™‡å—', 'ä¸´å¤', 'ç”˜å—', 'é’æµ·', 'è¥¿å®',
            'æµ·ä¸œ', 'æµ·åŒ—', 'é»„å—', 'æµ·å—', 'æœæ´›', 'ç‰æ ‘', 'æµ·è¥¿', 'å®å¤', 'é“¶å·', 'çŸ³å˜´å±±', 'å´å¿ ', 'å›ºåŸ', 'ä¸­å«',
            'æ–°ç–†', 'ä¹Œé²æœ¨é½', 'å…‹æ‹‰ç›ä¾', 'åé²ç•ª', 'å“ˆå¯†', 'æ˜Œå‰', 'åšå°”å¡”æ‹‰', 'å·´éŸ³éƒ­æ¥', 'é˜¿å…‹è‹', 'å…‹å­œå‹’è‹', 'å–€ä»€',
            'å’Œç”°', 'ä¼ŠçŠ', 'å¡”åŸ', 'é˜¿å‹’æ³°', 'ç›´è¾–å¸‚', 'è‡ªæ²»å·', 'è‡ªæ²»åŒº', 'å†…é™†', 'æ•™è‚²ç½‘', 'ç§‘ç ”ç½‘'],
		'é¦™æ¸¯': ['HK', 'HONG KONG', 'HONGKONG', 'é¦™æ¸¯', 'ğŸ‡­ğŸ‡°', 'HKG', 'é¦™ æ¸¯', 'HK-', 'HK_', 'é¦™æ¸¯ãƒ»', 'HK |', 'ğŸ‡¨ğŸ‡³é¦™æ¸¯',
             'HONG-KONG', '[HK]', '(HK)', 'HKèŠ‚ç‚¹', 'é¦™æ¸¯IEPL', 'HKIEPL', 'HK-IEPL', 'HONGKONGIEPL', 'hkèŠ‚ç‚¹',
             'é¦™æ¸¯èŠ‚ç‚¹'],
    'å°æ¹¾': ['TW', 'TAIWAN', 'å°ç£', 'å°æ¹¾', 'è‡ºç£', 'ğŸ‡¹ğŸ‡¼', 'TW-', 'å°åŒ—', 'è‡ºåŒ—', 'TAIPEI', 'TPE', 'TW_', 'TW.', 'TW|',
             '[TW]', 'å°æ¹¾èŠ‚ç‚¹', 'TWèŠ‚ç‚¹', 'ä¸­åç”µä¿¡', 'CHT', 'HINET'],
    'æ—¥æœ¬': ['JP', 'JAPAN', 'æ—¥æœ¬', 'æ±äº¬', 'TOKYO', 'å¤§é˜ª', 'OSAKA', 'ğŸ‡¯ğŸ‡µ', 'JP-', 'JP_', 'JPN', 'JAPAN', 'ä¸œäº¬',
             'å¤§å‚', '[JP]', 'æ—¥æœ¬èŠ‚ç‚¹', 'JPèŠ‚ç‚¹', 'åŸ¼ç‰', 'SAITAMA', 'åå¤å±‹', 'NAGOYA'],
    'æ–°åŠ å¡': ['SG', 'SINGAPORE', 'æ–°åŠ å¡', 'ğŸ‡¸ğŸ‡¬', 'SGP', 'SG-', 'SG_', 'Singapore', 'æ–°åŠ å¡èŠ‚ç‚¹', 'SGèŠ‚ç‚¹'],
    'éŸ©å›½': ['KR', 'KOREA', 'éŸ©å›½', 'é¦–å°”', 'SEOUL', 'ğŸ‡°ğŸ‡·', 'KR-', 'KR_', '[KR]', 'éŸ©åœ‹', 'éŸ©å›½èŠ‚ç‚¹', 'KRèŠ‚ç‚¹'],
		'ç¾å›½': ['US', 'USA', 'UNITED STATES', 'ç¾å›½', 'ç¾åœ‹', 'ç¾è¥¿', 'ç¾ä¸œ', 'ğŸ‡ºğŸ‡¸', 'US-', 'US_', '[US]', 'ç¾å›½èŠ‚ç‚¹',
             'USèŠ‚ç‚¹', 'çº½çº¦', 'æ´›æ‰çŸ¶', 'åœ£ä½•å¡', 'ç¡…è°·', 'åç››é¡¿', 'è¥¿é›…å›¾', 'èŠåŠ å“¥', 'è¾¾æ‹‰æ–¯', 'äºšç‰¹å…°å¤§', 'è¿ˆé˜¿å¯†',
             'NEW YORK', 'LOS ANGELES', 'SAN JOSE', 'SEATTLE', 'CHICAGO', 'DALLAS', 'ASHBURN'],
    'å¾·å›½': ['DE', 'GERMANY', 'å¾·å›½', 'ğŸ‡©ğŸ‡ª', 'DE-', 'DE_', '[DE]', 'GER-', 'å¾·å›½èŠ‚ç‚¹', 'DEèŠ‚ç‚¹', 'æ³•å…°å…‹ç¦',
             'FRANKFURT'],
    'è‹±å›½': ['GB', 'UK', 'ENGLAND', 'LONDON', 'è‹±å›½', 'è‹±æ ¼å…°', 'å€«æ•¦', 'ğŸ‡¬ğŸ‡§', 'UK-', 'UK_', '[UK]', 'GB-', 'ä¼¦æ•¦',
             'è‹±å›½èŠ‚ç‚¹', 'UKèŠ‚ç‚¹'],
    'ä¿„ç½—æ–¯': ['RU', 'RUSSIA', 'ä¿„ç½—æ–¯', 'ğŸ‡·ğŸ‡º', 'RU-', 'RU_', '[RU]', 'ä¿„ç¾…æ–¯', 'ä¿„ç½—æ–¯èŠ‚ç‚¹', 'RUèŠ‚ç‚¹', 'è«æ–¯ç§‘',
               'åœ£å½¼å¾—å ¡', 'ä¼¯åŠ›', 'æ–°è¥¿ä¼¯åˆ©äºš', 'MOSCOW'],
    'åŠ æ‹¿å¤§': ['CA', 'CANADA', 'åŠ æ‹¿å¤§', 'ğŸ‡¨ğŸ‡¦', 'CA-', 'CA_', '[CA]', 'åŠ æ‹¿å¤§èŠ‚ç‚¹', 'CAèŠ‚ç‚¹', 'å¤šä¼¦å¤š', 'æ¸©å“¥å',
               'è’™ç‰¹åˆ©å°”', 'TORONTO', 'VANCOUVER', 'MONTREAL', 'WATERLOO'],
    'æ¾³å¤§åˆ©äºš': ['AU', 'AUSTRALIA', 'æ¾³å¤§åˆ©äºš', 'æ¾³æ´²', 'ğŸ‡¦ğŸ‡º', 'AU-', 'AU_', '[AU]', 'æ¾³å¤§åˆ©äº',
                 'æ¾³å¤§åˆ©äºšèŠ‚ç‚¹', 'AUèŠ‚ç‚¹', 'æ‚‰å°¼', 'å¢¨å°”æœ¬', 'SYDNEY', 'MELBOURNE'],
    'æ¾³é—¨': ['MACAU', 'MACAO', 'æ¾³é—¨', 'ğŸ‡²ğŸ‡´', 'MO-', 'MO_'],
    'é©¬æ¥è¥¿äºš': ['MY', 'MALAYSIA', 'é©¬æ¥è¥¿äºš', 'å‰éš†å¡', 'KUALA LUMPUR', 'ğŸ‡²ğŸ‡¾', 'MY-', 'MY_', '[MY]', 'é¦¬ä¾†è¥¿äº',
                 'é©¬æ¥è¥¿äºšèŠ‚ç‚¹', 'MYèŠ‚ç‚¹'],
    'æ³°å›½': ['TH', 'THAILAND', 'æ³°å›½', 'æ›¼è°·', 'BANGKOK', 'ğŸ‡¹ğŸ‡­', 'TH-', 'TH_', '[TH]', 'æ³°åœ‹', 'æ³°å›½èŠ‚ç‚¹', 'THèŠ‚ç‚¹'],
    'è¶Šå—': ['VN', 'VIETNAM', 'è¶Šå—', 'æ²³å†…', 'HANOI', 'ğŸ‡»ğŸ‡³', 'VN-', 'VN_', '[VN]', 'è¶Šå—èŠ‚ç‚¹', 'VNèŠ‚ç‚¹'],
    'è²å¾‹å®¾': ['PH', 'PHILIPPINES', 'è²å¾‹å®¾', 'é©¬å°¼æ‹‰', 'MANILA', 'ğŸ‡µğŸ‡­', 'PH-', 'PH_', '[PH]', 'è²å¾‹è³“', 'è²å¾‹å®¾èŠ‚ç‚¹',
               'PHèŠ‚ç‚¹'],
    'å°åº¦å°¼è¥¿äºš': ['ID', 'INDONESIA', 'å°åº¦å°¼è¥¿äºš', 'é›…åŠ è¾¾', 'JAKARTA', 'ğŸ‡®ğŸ‡©', 'ID-', 'ID_', '[ID]', 'å°å°¼',
                   'å°åº¦å°¼è¥¿äºšèŠ‚ç‚¹', 'IDèŠ‚ç‚¹'],
    'å°åº¦': ['INDIA', 'å°åº¦', 'å­Ÿä¹°', 'MUMBAI', 'ğŸ‡®ğŸ‡³', 'IN-', 'IN_', '[IN]', 'å°åº¦èŠ‚ç‚¹', 'INèŠ‚ç‚¹', 'æ–°å¾·é‡Œ'],
    'æŸ¬åŸ”å¯¨': ['KH', 'CAMBODIA', 'æŸ¬åŸ”å¯¨', 'ğŸ‡°ğŸ‡­', 'KH-', 'KH_'],
    'åœŸè€³å…¶': ['TR', 'TURKEY', 'åœŸè€³å…¶', 'ä¼Šæ–¯å¦å¸ƒå°”', 'ISTANBUL', 'ğŸ‡¹ğŸ‡·', 'TR-', 'TR_', '[TR]', 'åœŸè€³å…¶èŠ‚ç‚¹', 'TRèŠ‚ç‚¹'],
    'é˜¿è”é…‹': ['AE', 'UAE', 'é˜¿è”é…‹', 'è¿ªæ‹œ', 'DUBAI', 'ğŸ‡¦ğŸ‡ª', 'AE-', 'AE_', '[AE]', 'é˜¿è¯é…‹', 'é˜¿è”é…‹èŠ‚ç‚¹', 'AEèŠ‚ç‚¹',
               'United Arab Emirates'],
    'æ²™ç‰¹é˜¿æ‹‰ä¼¯': ['SA', 'SAUDI ARABIA', 'æ²™ç‰¹', 'æ²™ç‰¹é˜¿æ‹‰ä¼¯', 'åˆ©é›…å¾—', 'RIYADH', 'ğŸ‡¸ğŸ‡¦', 'SA-', 'SA_', '[SA]',
                   'SAèŠ‚ç‚¹'],
    'å·´åŸºæ–¯å¦': ['PK', 'PAKISTAN', 'å·´åŸºæ–¯å¦', 'ğŸ‡µğŸ‡°', 'PK-', 'PK_', '[PK]', 'PKèŠ‚ç‚¹'],
    'ä»¥è‰²åˆ—': ['IL', 'ISRAEL', 'ä»¥è‰²åˆ—', 'è€¶è·¯æ’’å†·', 'JERUSALEM', 'ğŸ‡®ğŸ‡±', 'IL-', 'IL_'],
    'å¡å¡”å°”': ['QA', 'QATAR', 'å¡å¡”å°”', 'å¤šå“ˆ', 'DOHA', 'ğŸ‡¶ğŸ‡¦', 'QA-', 'QA_'],
    'å·´æ—': ['BAHRAIN', 'å·´æ—', 'ğŸ‡§ğŸ‡­', 'BH-', 'BH_'],
    'å­ŸåŠ æ‹‰å›½': ['BD', 'BANGLADESH', 'å­ŸåŠ æ‹‰', 'ğŸ‡§ğŸ‡©', 'BD-', 'BD_'],
    'å“ˆè¨å…‹æ–¯å¦': ['KZ', 'KAZAKHSTAN', 'å“ˆè¨å…‹æ–¯å¦', 'ğŸ‡°ğŸ‡¿', 'KZ-', 'KZ_'],
    'å‰å°”å‰æ–¯æ–¯å¦': ['KG', 'KYRGYZSTAN', 'å‰å°”å‰æ–¯æ–¯å¦', 'ğŸ‡°ğŸ‡¬', 'KG-', 'KG_'],
    'ä¹Œå…¹åˆ«å…‹æ–¯å¦': ['UZ', 'UZBEKISTAN', 'ä¹Œå…¹åˆ«å…‹æ–¯å¦', 'ğŸ‡ºğŸ‡¿', 'UZ-', 'UZ_'],
    'è’™å¤': ['MN', 'MONGOLIA', 'è’™å¤', 'ğŸ‡²ğŸ‡³', 'MN-', 'MN_'],
    'ç¼…ç”¸': ['MM', 'MYANMAR', 'ç¼…ç”¸', 'ğŸ‡²ğŸ‡²', 'MM-', 'MM_'],
    'å°¼æ³Šå°”': ['NP', 'NEPAL', 'å°¼æ³Šå°”', 'ğŸ‡³ğŸ‡µ', 'NP-', 'NP_'],
    'è€æŒ': ['LA', 'LAOS', 'è€æŒ', 'ğŸ‡±ğŸ‡¦', 'LA-', 'LA_'],
    'æ–‡è±': ['BN', 'BRUNEI', 'æ–‡è±', 'ğŸ‡§ğŸ‡³', 'BN-', 'BN_'],
    'çº¦æ—¦': ['JO', 'JORDAN', 'çº¦æ—¦', 'ğŸ‡¯ğŸ‡´', 'JO-', 'JO_'],
    'é»å·´å«©': ['LB', 'LEBANON', 'é»å·´å«©', 'ğŸ‡±ğŸ‡§', 'LB-', 'LB_'],
    'é˜¿æ›¼': ['OM', 'OMAN', 'é˜¿æ›¼', 'ğŸ‡´ğŸ‡²', 'OM-', 'OM_'],
    'æ ¼é²å‰äºš': ['GE', 'GEORGIA', 'æ ¼é²å‰äºš', 'ğŸ‡¬ğŸ‡ª', 'GE-', 'GE_'],
    'äºšç¾å°¼äºš': ['AM', 'ARMENIA', 'äºšç¾å°¼äºš', 'ğŸ‡¦ğŸ‡²', 'AM-', 'AM_'],
    'é˜¿å¡æ‹œç–†': ['AZ', 'AZERBAIJAN', 'é˜¿å¡æ‹œç–†', 'ğŸ‡¦ğŸ‡¿', 'AZ-', 'AZ_'],
    'å™åˆ©äºš': ['SY', 'SYRIA', 'å™åˆ©äºš', 'ğŸ‡¸ğŸ‡¾', 'SY-', 'SY_'],
    'ä¼Šæ‹‰å…‹': ['IQ', 'IRAQ', 'ä¼Šæ‹‰å…‹', 'ğŸ‡®ğŸ‡¶', 'IQ-', 'IQ_'],
    'ä¼Šæœ—': ['IR', 'IRAN', 'ä¼Šæœ—', 'ğŸ‡®ğŸ‡·', 'IR-', 'IR_'],
    'é˜¿å¯Œæ±—': ['AF', 'AFGHANISTAN', 'é˜¿å¯Œæ±—', 'ğŸ‡¦ğŸ‡«', 'AF-', 'AF_'],
    'å¢¨è¥¿å“¥': ['MX', 'MEXICO', 'å¢¨è¥¿å“¥', 'ğŸ‡²ğŸ‡½', 'MX-', 'MX_', '[MX]', 'å¢¨è¥¿å“¥èŠ‚ç‚¹', 'MXèŠ‚ç‚¹'],
    'å·´è¥¿': ['BR', 'BRAZIL', 'å·´è¥¿', 'ğŸ‡§ğŸ‡·', 'BR-', 'BR_', '[BR]', 'å·´è¥¿èŠ‚ç‚¹', 'BRèŠ‚ç‚¹', 'åœ£ä¿ç½—', 'SAO PAULO'],
    'é˜¿æ ¹å»·': ['AR', 'ARGENTINA', 'é˜¿æ ¹å»·', 'ğŸ‡¦ğŸ‡·', 'AR-', 'AR_'],
    'æ™ºåˆ©': ['CL', 'CHILE', 'æ™ºåˆ©', 'ğŸ‡¨ğŸ‡±', 'CL-', 'CL_'],
    'å“¥ä¼¦æ¯”äºš': ['CO', 'COLOMBIA', 'å“¥ä¼¦æ¯”äºš', 'ğŸ‡¨ğŸ‡´', 'CO-', 'CO_'],
    'ç§˜é²': ['PE', 'PERU', 'ç§˜é²', 'ğŸ‡µğŸ‡ª', 'PE-', 'PE_'],
    'å§”å†…ç‘æ‹‰': ['VE', 'VENEZUELA', 'å§”å†…ç‘æ‹‰', 'ğŸ‡»ğŸ‡ª', 'VE-', 'VE_'],
    'å„ç“œå¤šå°”': ['EC', 'ECUADOR', 'å„ç“œå¤šå°”', 'ğŸ‡ªğŸ‡¨', 'EC-', 'EC_'],
    'ä¹Œæ‹‰åœ­': ['UY', 'URUGUAY', 'ä¹Œæ‹‰åœ­', 'ğŸ‡ºğŸ‡¾', 'UY-', 'UY_'],
    'å·´æ‹‰åœ­': ['PY', 'PARAGUAY', 'å·´æ‹‰åœ­', 'ğŸ‡µğŸ‡¾', 'PY-', 'PY_'],
    'ç»åˆ©ç»´äºš': ['BO', 'BOLIVIA', 'ç»åˆ©ç»´äºš', 'ğŸ‡§ğŸ‡´', 'BO-', 'BO_'],
    'å“¥æ–¯è¾¾é»åŠ ': ['CR', 'COSTA RICA', 'å“¥æ–¯è¾¾é»åŠ ', 'ğŸ‡¨ğŸ‡·', 'CR-', 'CR_'],
    'å·´æ‹¿é©¬': ['PA', 'PANAMA', 'å·´æ‹¿é©¬', 'ğŸ‡µğŸ‡¦', 'PA-', 'PA_'],
    'æ³•å›½': ['FR', 'FRANCE', 'æ³•å›½', 'ğŸ‡«ğŸ‡·', 'FR-', 'FR_', '[FR]', 'æ³•å›½èŠ‚ç‚¹', 'FRèŠ‚ç‚¹', 'å·´é»', 'PARIS'],
    'è·å…°': ['NL', 'NETHERLANDS', 'è·å…°', 'ğŸ‡³ğŸ‡±', 'NL-', 'NL_', '[NL]', 'è·è˜­', 'è·å…°èŠ‚ç‚¹', 'NLèŠ‚ç‚¹', 'é˜¿å§†æ–¯ç‰¹ä¸¹',
             'AMSTERDAM'],
    'ç‘å£«': ['CH', 'SWITZERLAND', 'ç‘å£«', 'ğŸ‡¨ğŸ‡­', 'CH-', 'CH_', '[CH]', 'ç‘å£«èŠ‚ç‚¹', 'CHèŠ‚ç‚¹', 'è‹é»ä¸–', 'ZURICH'],
    'æ„å¤§åˆ©': ['ITALY', 'æ„å¤§åˆ©', 'ğŸ‡®ğŸ‡¹', 'IT-', 'IT_', '[IT]', 'æ„å¤§åˆ©èŠ‚ç‚¹', 'ITèŠ‚ç‚¹', 'ç±³å…°', 'MILAN'],
    'è¥¿ç­ç‰™': ['ES', 'SPAIN', 'è¥¿ç­ç‰™', 'ğŸ‡ªğŸ‡¸', 'ES-', 'ES_', '[ES]', 'è¥¿ç­ç‰™èŠ‚ç‚¹', 'ESèŠ‚ç‚¹', 'é©¬å¾·é‡Œ', 'MADRID'],
    'ç‘å…¸': ['SE', 'SWEDEN', 'ç‘å…¸', 'ğŸ‡¸ğŸ‡ª', 'SE-', 'SE_', '[SE]', 'ç‘å…¸èŠ‚ç‚¹', 'SEèŠ‚ç‚¹'],
    'èŠ¬å…°': ['FI', 'FINLAND', 'èŠ¬å…°', 'ğŸ‡«ğŸ‡®', 'FI-', 'FI_', '[FI]', 'èŠ¬è˜­', 'èŠ¬å…°èŠ‚ç‚¹', 'FIèŠ‚ç‚¹'],
    'çˆ±å°”å…°': ['IE', 'IRELAND', 'çˆ±å°”å…°', 'ğŸ‡®ğŸ‡ª', 'IE-', 'IE_', '[IE]', 'æ„›çˆ¾è˜­', 'çˆ±å°”å…°èŠ‚ç‚¹', 'IEèŠ‚ç‚¹', 'éƒ½æŸæ—',
               'DUBLIN'],
    'æŒªå¨': ['NO', 'NORWAY', 'æŒªå¨', 'ğŸ‡³ğŸ‡´', 'NO-', 'NO_', '[NO]', 'æŒªå¨èŠ‚ç‚¹', 'NOèŠ‚ç‚¹', 'å¥¥æ–¯é™†', 'OSLO'],
    'ä¸¹éº¦': ['DK', 'DENMARK', 'ä¸¹éº¦', 'ğŸ‡©ğŸ‡°', 'DK-', 'DK_', '[DK]', 'ä¸¹éº¥', 'ä¸¹éº¦èŠ‚ç‚¹', 'DKèŠ‚ç‚¹'],
    'å¥¥åœ°åˆ©': ['AUSTRIA', 'å¥¥åœ°åˆ©', 'ğŸ‡¦ğŸ‡¹', 'AT-', 'AT_', '[AT]', 'å¥§åœ°åˆ©', 'å¥¥åœ°åˆ©èŠ‚ç‚¹', 'ATèŠ‚ç‚¹'],
    'æ³¢å…°': ['PL', 'POLAND', 'æ³¢å…°', 'ğŸ‡µğŸ‡±', 'PL-', 'PL_', 'åæ²™', 'WARSAW'],
    'æ¯”åˆ©æ—¶': ['BE', 'BELGIUM', 'æ¯”åˆ©æ—¶', 'ğŸ‡§ğŸ‡ª', 'BE-', 'BE_'],
    'æ·å…‹': ['CZ', 'CZECH', 'æ·å…‹', 'ğŸ‡¨ğŸ‡¿', 'CZ-', 'CZ_'],
    'åŒˆç‰™åˆ©': ['HU', 'HUNGARY', 'åŒˆç‰™åˆ©', 'ğŸ‡­ğŸ‡º', 'HU-', 'HU_'],
    'ç½—é©¬å°¼äºš': ['RO', 'ROMANIA', 'ç½—é©¬å°¼äºš', 'ğŸ‡·ğŸ‡´', 'RO-', 'RO_'],
    'ä¹Œå…‹å…°': ['UA', 'UKRAINE', 'ä¹Œå…‹å…°', 'ğŸ‡ºğŸ‡¦', 'UA-', 'UA_'],
    'å¸Œè…Š': ['GR', 'GREECE', 'å¸Œè…Š', 'ğŸ‡¬ğŸ‡·', 'GR-', 'GR_'],
    'è‘¡è„ç‰™': ['PT', 'PORTUGAL', 'è‘¡è„ç‰™', 'ğŸ‡µğŸ‡¹', 'PT-', 'PT_'],
    'ä¿åŠ åˆ©äºš': ['BG', 'BULGARIA', 'ä¿åŠ åˆ©äºš', 'ğŸ‡§ğŸ‡¬', 'BG-', 'BG_'],
    'å…‹ç½—åœ°äºš': ['HR', 'CROATIA', 'å…‹ç½—åœ°äºš', 'ğŸ‡­ğŸ‡·', 'HR-', 'ğŸ‡­ğŸ‡·_'],
    'çˆ±æ²™å°¼äºš': ['EE', 'ESTONIA', 'çˆ±æ²™å°¼äºš', 'ğŸ‡ªğŸ‡ª', 'EE-', 'EE_'],
    'å†°å²›': ['ICELAND', 'å†°å²›', 'ğŸ‡®ğŸ‡¸', 'IS-', 'IS_'],
    'æ‹‰è„±ç»´äºš': ['LV', 'LATVIA', 'æ‹‰è„±ç»´äºš', 'ğŸ‡±ğŸ‡»', 'LV-', 'LV_'],
    'ç«‹é™¶å®›': ['LT', 'LITHUANIA', 'ç«‹é™¶å®›', 'ğŸ‡±ğŸ‡¹', 'LT-', 'LT_'],
    'å¢æ£®å ¡': ['LU', 'LUXEMBOURG', 'å¢æ£®å ¡', 'ğŸ‡±ğŸ‡º', 'LU-', 'LU_'],
    'å¡å°”ç»´äºš': ['RS', 'SERBIA', 'å¡å°”ç»´äºš', 'ğŸ‡·ğŸ‡¸', 'RS-', 'RS_'],
    'æ–¯æ´›ä¼å…‹': ['SK', 'SLOVAKIA', 'æ–¯æ´›ä¼å…‹', 'ğŸ‡¸ğŸ‡°', 'SK-', 'SK_'],
    'æ–¯æ´›æ–‡å°¼äºš': ['SI', 'SLOVENIA', 'æ–¯æ´›æ–‡å°¼äºš', 'ğŸ‡¸ğŸ‡®', 'SI-', 'SI_'],
    'é˜¿å°”å·´å°¼äºš': ['AL', 'ALBANIA', 'é˜¿å°”å·´å°¼äºš', 'ğŸ‡¦ğŸ‡±', 'AL-', 'AL_'],
    'æ‘©å°”å¤šç“¦': ['MD', 'MOLDOVA', 'æ‘©å°”å¤šç“¦', 'ğŸ‡²ğŸ‡©', 'MD-', 'MD_'],
    'æ³¢æ–¯å°¼äºš': ['BA', 'BOSNIA', 'æ³¢é»‘', 'ğŸ‡§ğŸ‡¦', 'BA-', 'BA_'],
    'ç™½ä¿„ç½—æ–¯': ['BY', 'BELARUS', 'ç™½ä¿„ç½—æ–¯', 'ğŸ‡§ğŸ‡¾', 'BY-', 'BY_'],
    'å¡æµ¦è·¯æ–¯': ['CY', 'CYPRUS', 'å¡æµ¦è·¯æ–¯', 'ğŸ‡¨ğŸ‡¾', 'CY-', 'CY_'],
    'é©¬è€³ä»–': ['MT', 'MALTA', 'é©¬è€³ä»–', 'ğŸ‡²ğŸ‡¹', 'MT-', 'MT_'],
    'æ‘©çº³å“¥': ['MC', 'MONACO', 'æ‘©çº³å“¥', 'ğŸ‡²ğŸ‡¨', 'MC-', 'MC_'],
    'åˆ—æ”¯æ•¦å£«ç™»': ['LI', 'LIECHTENSTEIN', 'åˆ—æ”¯æ•¦å£«ç™»', 'ğŸ‡±ğŸ‡®', 'LI-', 'LI_'],
    'é»‘å±±': ['ME', 'MONTENEGRO', 'é»‘å±±', 'ğŸ‡²ğŸ‡ª', 'ME-', 'ME_'],
    'é©¬å…¶é¡¿': ['MK', 'MACEDONIA', 'é©¬å…¶é¡¿', 'ğŸ‡²ğŸ‡°', 'MK-', 'MK_'],
    'æ–°è¥¿å…°': ['NZ', 'NEW ZEALAND', 'æ–°è¥¿å…°', 'ğŸ‡³ğŸ‡¿', 'NZ-', 'NZ_', '[NZ]', 'æ–°è¥¿è˜­', 'æ–°è¥¿å…°èŠ‚ç‚¹', 'NZèŠ‚ç‚¹', 'å¥¥å…‹å…°',
               'AUCKLAND'],
    'æ–æµ': ['FJ', 'FIJI', 'æ–æµ', 'ğŸ‡«ğŸ‡¯', 'FJ-', 'FJ_'],
    'å—é': ['ZA', 'SOUTH AFRICA', 'å—é', 'ğŸ‡¿ğŸ‡¦', 'ZA-', 'ZA_', '[ZA]', 'å—éèŠ‚ç‚¹', 'ZAèŠ‚ç‚¹', 'çº¦ç¿°å†…æ–¯å ¡',
             'JOHANNESBURG'],
    'åŸƒåŠ': ['EG', 'EGYPT', 'åŸƒåŠ', 'ğŸ‡ªğŸ‡¬', 'EG-', 'EG_'],
    'å°¼æ—¥åˆ©äºš': ['NG', 'NIGERIA', 'å°¼æ—¥åˆ©äºš', 'ğŸ‡³ğŸ‡¬', 'NG-', 'NG_'],
    'è‚¯å°¼äºš': ['KE', 'KENYA', 'è‚¯å°¼äºš', 'ğŸ‡°ğŸ‡ª', 'KE-', 'KE_'],
    'åŠ çº³': ['GH', 'GHANA', 'åŠ çº³', 'ğŸ‡¬ğŸ‡­', 'GH-', 'GH_'],
    'æ‘©æ´›å“¥': ['MA', 'MOROCCO', 'æ‘©æ´›å“¥', 'ğŸ‡²ğŸ‡¦', 'MA-', 'MA_'],
    'é˜¿å°”åŠåˆ©äºš': ['DZ', 'ALGERIA', 'é˜¿å°”åŠåˆ©äºš', 'ğŸ‡©ğŸ‡¿', 'DZ-', 'AL_'],
    'å®‰å“¥æ‹‰': ['AO', 'ANGOLA', 'å®‰å“¥æ‹‰', 'ğŸ‡¦ğŸ‡´', 'AO-', 'AO_'],
    'çªå°¼æ–¯': ['TN', 'TUNISIA', 'çªå°¼æ–¯', 'ğŸ‡¹ğŸ‡³', 'TN-', 'TN_'],
    'æ¯›é‡Œæ±‚æ–¯': ['MU', 'MAURITIUS', 'æ¯›é‡Œæ±‚æ–¯', 'ğŸ‡²ğŸ‡º', 'MU-', 'MU_'],
    'ç›´è¿': ['ç›´è¿', 'DIRECT'],
    'ä¸­è½¬': ['ä¸­è½¬', 'RELAY', 'TRANSFER', 'éš§é“', 'TUNNEL', 'å…¬ç½‘ä¸­è½¬', 'æµ·å¤–', 'å›½å†…', 'å…¥å£', 'å‡ºå£'],
    'ä¸“çº¿': ['ä¸“çº¿', 'IPLC', 'IEPL', 'ä¸“', 'å†…ç½‘', 'SD-WAN', 'PRIVATE LINE'],
    'BGP': ['BGP'],
    'CDN': ['CDN'],
    'æœªçŸ¥': ['æœªçŸ¥', 'UNKNOWN'],
}
ORDERED_COUNTRIES = list(COUNTRY_KEYWORDS.keys())


# --- è¾…åŠ©å‡½æ•° ---
def safe_html(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return html.escape(text)


async def send_notification_to_admin(bot: Bot, update: Update, results: list):
    """[ä¿®æ”¹2] æ ¼å¼åŒ–è¯¦ç»†ä¿¡æ¯å¹¶æ¨é€åˆ°ç®¡ç†å‘˜çš„åŠŸèƒ½"""
    try:
        user = update.effective_user
        chat = update.effective_chat
        message = update.effective_message

        user_info_str = f"<b>ç”¨æˆ·:</b> {safe_html(user.full_name)}"
        if user.username:
            user_info_str += f" (@{safe_html(user.username)})"
        user_info_str += f" (ID: <code>{user.id}</code>)"

        if chat.type == "private":
            chat_info_str = "<b>æ¥æº:</b> <code>ç§èŠ</code>"
        else:
            chat_link = message.link
            chat_title_safe = safe_html(chat.title or 'æœªçŸ¥ç¾¤ç»„')
            chat_info_str = f"<b>æ¥æºç¾¤ç»„:</b> <a href='{chat_link}'>{chat_title_safe}</a> (ID: <code>{chat.id}</code>)"

        header_info = f"<b>è®¢é˜…æŸ¥è¯¢é€šçŸ¥</b>\n\n{user_info_str}\n{chat_info_str}\n"

        valid_results = [r for r in results if
                         r and r.get('status', {}).get('valid') and not r.get('status', {}).get('exhausted') and not
                         r.get('status', {}).get('expired')]

        is_doc_from_user = update.message.document is not None

        if is_doc_from_user and len(valid_results) > 5:
            summary_text = header_info + f"\nç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶äº§ç”Ÿäº† {len(valid_results)} ä¸ªæœ‰æ•ˆè®¢é˜…ï¼Œè¯¦æƒ…è§é™„ä»¶ã€‚"
            await bot.send_message(chat_id=NOTIFICATION_ID, text=summary_text, parse_mode="HTML")

            def clean_html_for_file(html_text: str) -> str:
                # 1. å…ˆå°† <pre><code> è¿™ç§ç‰¹æ®Šç»“æ„è½¬æ¢æˆæ¢è¡Œ
                text_with_newlines = html_text.replace('<pre><code>', '\n').replace('</code></pre>', '')
                # 2. ä½¿ç”¨é€šç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œç§»é™¤æ‰€æœ‰ <...> å½¢å¼çš„HTMLæ ‡ç­¾
                plain_text = re.sub(r'<[^>]+>', '', text_with_newlines)
                # 3. å¤„ç†åƒ &amp; è¿™æ ·çš„HTMLå®ä½“ç¼–ç ï¼Œå¹¶å»é™¤é¦–å°¾å¤šä½™çš„ç©ºç™½
                return html.unescape(plain_text).strip()

            file_content = "\n\n".join(clean_html_for_file(r['summary_text']) for r in valid_results).encode('utf-8')
            await bot.send_document(
                chat_id=NOTIFICATION_ID,
                document=InputFile(io.BytesIO(file_content), filename="æœ‰æ•ˆè®¢é˜….txt"),
                caption=f"æ¥è‡ªç”¨æˆ· {user.id} çš„æœ‰æ•ˆè®¢é˜…"
            )
        else:
            if not valid_results: return

            details_text = "\n\n".join([r['summary_text'] for r in valid_results])
            final_message = header_info + "\n<b>æœ‰æ•ˆè®¢é˜…è¯¦æƒ…:</b>\n" + details_text

            if len(final_message) > 4096:
                cutoff = 4096 - 100
                final_message = final_message[:cutoff] + "\n\n...(æ¶ˆæ¯è¿‡é•¿ï¼Œå·²æˆªæ–­)"

            await bot.send_message(
                chat_id=NOTIFICATION_ID,
                text=final_message,
                parse_mode="HTML",
                disable_web_page_preview=True
            )

    except Exception as e:
        print(f"å‘é€é€šçŸ¥åˆ°ç®¡ç†å‘˜å¤±è´¥ (ID: {NOTIFICATION_ID}): {e}")
        try:
            await bot.send_message(chat_id=NOTIFICATION_ID,
                                   text=f"å¤„ç†æ¥è‡ªç”¨æˆ· {update.effective_user.id} çš„æŸ¥è¯¢å¹¶å‘æ‚¨å‘é€è¯¦ç»†é€šçŸ¥æ—¶å‡ºé”™: {e}")
        except Exception as fallback_e:
            print(f"å‘é€å›é€€é€šçŸ¥å¤±è´¥: {fallback_e}")


def format_time_remaining(seconds: int) -> str:
    if seconds is None or seconds < 0:
        return "æœªçŸ¥"
    d, remainder = divmod(seconds, 86400)
    h, remainder = divmod(remainder, 3600)
    m, _ = divmod(remainder, 60)
    return f"{int(d)}å¤©{int(h)}å°æ—¶{int(m)}åˆ†é’Ÿ"


def format_traffic(b: int) -> str:
    if b is None: return "N/A"
    units = ["B", "KB", "MB", "GB", "TB", "PB"]
    i = 0
    while b >= 1024 and i < len(units) - 1:
        b /= 1024
        i += 1
    return f"{b:.2f}{units[i]}"


def gen_bar(pct: float, length: int = 12) -> str:
    full = int(pct / 100 * length)
    return f"[{'â¬¢' * full}{'â¬¡' * (length - full)}]"


def _format_progress_bar(current: int, total: int) -> str:
    if total == 0:
        return "æŸ¥è¯¢/è§£æä¸­... [â¬¡â¬¡â¬¡â¬¡â¬¡â¬¡â¬¡â¬¡â¬¡â¬¡] 0% (0/0)"

    percentage = (current / total) * 100
    bar_length = 12
    filled_length = int(bar_length * current // total)
    bar = 'â¬¢' * filled_length + 'â¬¡' * (bar_length - filled_length)
    return f"æŸ¥è¯¢/è§£æä¸­... [{bar}] {percentage:.0f}% ({current}/{total})"


def get_timestamped_urls(cache_file: str, new_urls: list[str]) -> list[str]:
    existing_urls_map = {}
    try:
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line: continue
                    match = re.search(r'\]\s*(https?://.*)', line)
                    if match:
                        url = match.group(1).strip()
                        existing_urls_map[url] = line
                    else:
                        existing_urls_map[line] = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {line}"
    except IOError as e:
        print(f"è¯»å–ç¼“å­˜æ–‡ä»¶æ—¶å‡ºé”™ {cache_file}: {e}")

    current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    for url in new_urls:
        url = url.strip()
        if url: existing_urls_map[url] = f"[{current_timestamp}] {url}"
    return sorted(existing_urls_map.values(), key=lambda item: item.split('] ')[-1])


def update_cache_file(new_urls: list[str]):
    all_timestamped_lines = get_timestamped_urls(CACHE_FILE, new_urls)
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            for line in all_timestamped_lines:
                f.write(line + '\n')
    except IOError as e:
        print(f"å†™å…¥ç¼“å­˜æ–‡ä»¶æ—¶å‡ºé”™ {CACHE_FILE}: {e}")


def update_invalid_cache_file(new_urls: list[str]):
    all_timestamped_lines = get_timestamped_urls(INVALID_CACHE_FILE, new_urls)
    try:
        with open(INVALID_CACHE_FILE, 'w', encoding='utf-8') as f:
            for line in all_timestamped_lines:
                f.write(line + '\n')
    except IOError as e:
        print(f"å†™å…¥æ— æ•ˆç¼“å­˜æ–‡ä»¶æ—¶å‡ºé”™ {INVALID_CACHE_FILE}: {e}")


def update_valid_nodes_file(new_nodes: list[str]):
    existing_nodes = set()
    try:
        if os.path.exists(VALID_NODES_FILE):
            with open(VALID_NODES_FILE, 'r', encoding='utf-8') as f:
                existing_nodes.update(line.strip() for line in f if line.strip())
    except IOError as e:
        print(f"è¯»å–æœ‰æ•ˆèŠ‚ç‚¹æ–‡ä»¶æ—¶å‡ºé”™ {VALID_NODES_FILE}: {e}")

    existing_nodes.update(node.strip() for node in new_nodes if node.strip())
    try:
        with open(VALID_NODES_FILE, 'w', encoding='utf-8') as f:
            for node in sorted(list(existing_nodes)):
                f.write(node + '\n')
    except IOError as e:
        print(f"å†™å…¥æœ‰æ•ˆèŠ‚ç‚¹æ–‡ä»¶æ—¶å‡ºé”™ {VALID_NODES_FILE}: {e}")


async def try_get_filename_from_header_async(session: ClientSession, download_url: str) -> Optional[str]:
    headers_browser = {"User-Agent": "Mozilla/5.0"}
    try:
        async with session.get(download_url, timeout=10, headers=headers_browser) as r:
            if r.status == 200:
                cd = r.headers.get('Content-Disposition')
                if cd:
                    m1 = RE_FILENAME_UTF8.search(cd)
                    if m1: return unquote(m1.group(1)).replace('%20', ' ').replace('%2B', '+')
                    m2 = RE_FILENAME_SIMPLE.search(cd)
                    if m2: return unquote(m2.group(1)).replace('%20', ' ').replace('%2B', '+')
    except Exception:
        pass
    return None


async def try_get_title_from_html_async(session: ClientSession, base_url: str) -> str:
    headers_browser = {"User-Agent": "Mozilla/5.0"}
    try:
        content = None
        async with session.get(base_url + "/auth/login", headers=headers_browser, timeout=10) as resp:
            if resp.status != 200:
                async with session.get(base_url, headers=headers_browser, timeout=5) as resp_main:
                    content = await resp_main.read()
            else:
                content = await resp.read()

        soup = BeautifulSoup(content, "html.parser")
        title = soup.title.string.strip() if soup.title and soup.title.string else 'æœªçŸ¥'

        if "Cloudflare" in title: return 'è¯¥åŸŸåä»…é™å›½å†…IPè®¿é—®'
        if "Access denied" in title or "404" in title: return 'è¯¥åŸŸåéæœºåœºé¢æ¿åŸŸå'
        if "Just a moment" in title: return 'è¯¥åŸŸåå¼€å¯äº†5sç›¾'
        return title.replace("ç™»å½• â€” ", "")
    except Exception:
        return 'æœªçŸ¥'


async def get_filename_from_url_async(session: ClientSession, url: str) -> str:
    if "sub?target=" in url:
        inner_match = re.search(r"url=([^&]*)", url)
        if inner_match:
            return await get_filename_from_url_async(session, unquote(inner_match.group(1)))

    if "api/v1/client/subscribe?token" in url:
        if "&flag=clash" not in url: url += "&flag=clash"
        name = await try_get_filename_from_header_async(session, url)
        return name if name else 'æœªçŸ¥'
    try:
        parsed = urlparse(url)
        domain = f"{parsed.scheme}://{parsed.hostname}"
        return await try_get_title_from_html_async(session, domain)
    except Exception:
        return 'æœªçŸ¥'


def extract_node_name(proxy: dict) -> str:
    for k in ["name", "ps", "desc", "remarks", "remark"]:
        if isinstance(proxy, dict) and k in proxy and proxy[k]:
            return str(proxy[k])
    return ""


def extract_quota_and_expire(node_name: str):
    quota_pat = re.compile(r'([\d\.]+ ?[MGTP]B?) ?\| ?([\d\.]+ ?[MGTP]B?)', re.I)
    expire_pats = [
        re.compile(r'Expire Date[:ï¼š ]+(\d{4}/\d{2}/\d{2})', re.I),
        re.compile(r'åˆ°æœŸ[æ—¥|æ—¶é—´|æ—¥æœŸ|è‡³][:ï¼š ]*(\d{4}[-/]\d{2}[-/]\d{2})', re.I),
        re.compile(r'(\d{4}[-/]\d{2}[-/]\d{2})', re.I),
    ]
    quota, expire = None, None
    m = quota_pat.search(node_name)
    if m: quota = f"{m.group(1)} / {m.group(2)}"
    for pat in expire_pats:
        m = pat.search(node_name)
        if m:
            expire = m.group(1)
            break
    return quota, expire


def scan_proxies_quota_expire(proxies):
    quotas, expires = set(), set()
    for p in proxies:
        name = p.get('name', '') if isinstance(p, dict) else str(p)
        quota, expire = extract_quota_and_expire(name)
        if quota: quotas.add(quota)
        if expire: expires.add(expire)
    return list(quotas), list(expires)


def try_yaml_parse(text_content):
    text_content = text_content.lstrip('\ufeff').strip()
    if not text_content or ('proxies:' not in text_content and 'proxy-providers:' not in text_content):
        return []
    if _HAS_RUAMEL:
        try:
            from io import StringIO
            return list(yaml_ruamel.load_all(StringIO(text_content)))
        except Exception:
            pass
    try:
        return list(yaml.safe_load_all(text_content))
    except Exception:
        return []


def is_base64_string(s):
    return bool(re.fullmatch(r'[A-Za-z0-9+/=_-]{16,}', s))


def safe_b64decode(s):
    s = s.strip().replace('\r', '').replace('\n', '')
    if not is_base64_string(s): return None
    padding = len(s) % 4
    if padding != 0: s += "=" * (4 - padding)
    try:
        return base64.urlsafe_b64decode(s.encode()).decode('utf-8', errors='ignore')
    except (binascii.Error, ValueError):
        return None


def find_proxies_in_config(data):
    if isinstance(data, dict):
        if 'proxies' in data and isinstance(data['proxies'], list):
            return data['proxies']
        for value in data.values():
            found_proxies = find_proxies_in_config(value)
            if found_proxies is not None:
                return found_proxies
    return None


def parse_node_lines_with_b64(lines_to_parse, node_info, max_depth=3, current_depth=0):
    if current_depth > max_depth: return
    for line in lines_to_parse:
        stripped_line = line.strip()
        if not stripped_line: continue
        if stripped_line.lower().startswith(('http://', 'https://')):
            try:
                parsed_u = urlparse(stripped_line)
                if (parsed_u.path and parsed_u.path != '/') or parsed_u.query: continue
            except Exception:
                continue

        if len(stripped_line) > 20 and not RE_NODE_LINKS.match(stripped_line):
            decoded = safe_b64decode(stripped_line)
            if decoded:
                parse_node_lines_with_b64(decoded.splitlines(), node_info, max_depth, current_depth + 1)
                continue

        if RE_NODE_LINKS.match(stripped_line):
            node_info["node_count"] += 1
            if "all_node_links" in node_info: node_info["all_node_links"].append(stripped_line)
            protocol_type = stripped_line.split("://", 1)[0].upper()
            node_info["protocol_types"].add(protocol_type)
            
            node_name = ""
            try:
                name_match = re.search(r'#(.*)', stripped_line)
                if name_match:
                    node_name = unquote(name_match.group(1), 'utf-8', 'ignore').replace('+', ' ')
                else:
                    if protocol_type == 'VMESS':
                        base64_part = stripped_line.split('://', 1)[1]
                        base64_part = base64_part.split('?', 1)[0]
                        decoded_json_str = safe_b64decode(base64_part)
                        if decoded_json_str:
                            try:
                                vmess_config = json.loads(decoded_json_str)
                                node_name = vmess_config.get('ps', '')
                            except (json.JSONDecodeError, TypeError):
                                pass

                    if not node_name:
                        qs = urlparse(stripped_line).query
                        if qs:
                            params = dict(parse_qsl(qs))
                            node_name = params.get('remark', '') or params.get('name', '')
            except Exception:
                node_name = "æ— æ³•è§£æåç§°"

            final_node_name = node_name or "æ— æ³•è§£æåç§°"
            node_info["all_nodes_list"].append({"name": final_node_name, "protocol": protocol_type})
            country = SubscriptionBot._extract_country_from_name_static(final_node_name)
            if country != "æœªçŸ¥": node_info["countries"].add(country)


class SubscriptionBot:
    def __init__(self):
        self.domain_map = {}
        self._sem = asyncio.Semaphore(CONCURRENT_LIMIT)
        self.load_domain_map()
        self._name_cache = {}
        self._node_info_cache = {}

    def load_domain_map(self):
        self.domain_map = {}
        try:
            if not os.path.exists(DOMAIN_MAP_FILE):
                with open(DOMAIN_MAP_FILE, "w", encoding="utf-8") as f: pass
                return
            with open(DOMAIN_MAP_FILE, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#') or "=" not in line: continue
                    try:
                        d, n = line.split("=", 1)
                        if d.strip() and n.strip(): self.domain_map[d.strip()] = n.strip()
                    except ValueError:
                        continue
        except Exception as e:
            print(f"åŠ è½½åŸŸåæ˜ å°„æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def save_domain_map(self):
        try:
            with open(DOMAIN_MAP_FILE, "w", encoding="utf-8") as f:
                for d, n in self.domain_map.items():
                    f.write(f"{d}={n}\n")
        except IOError as e:
            print(f"å†™å…¥åŸŸåæ˜ å°„æ–‡ä»¶æ—¶å‘ç”ŸI/Oé”™è¯¯: {e}")

    async def fetch_airport_name_from_response_header(self, session: ClientSession, url: str) -> str:
        parsed_url = urlparse(url)
        query_params = dict(parse_qsl(parsed_url.query))
        query_params['flag'] = 'clash'
        new_query = urlencode(query_params)
        url = urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query, parsed_url.fragment))
        try:
            async with session.get(url, timeout=10) as r:
                if r.status == 200:
                    cd = r.headers.get('Content-Disposition')
                    if cd:
                        m1 = RE_FILENAME_UTF8.search(cd)
                        if m1: return unquote(m1.group(1)).rsplit('.', 1)[0]
                        m2 = RE_FILENAME_SIMPLE.search(cd)
                        if m2: return unquote(m2.group(1)).rsplit('.', 1)[0]
                return 'æœªè¯†åˆ«çš„æ–‡ä»¶åæ ¼å¼'
        except Exception:
            return 'è¯·æ±‚å¼‚å¸¸'

    async def extract_name(self, session: ClientSession, url: str) -> str:
        if url in self._name_cache: return self._name_cache[url]
        host = urlparse(url).hostname or ""
        for key, val in self.domain_map.items():
            if key in host:
                self._name_cache[url] = val
                return val
        name = await self.fetch_airport_name_from_response_header(session, url)
        if not name.startswith(("è¯·æ±‚", "æœªæ‰¾åˆ°", "æœªè¯†åˆ«")):
            self._name_cache[url] = name
            return name
        name = await get_filename_from_url_async(session, url)
        self._name_cache[url] = name
        return name

    @staticmethod
    def _extract_country_from_name_static(node_name: str) -> str:
        upper_name = node_name.upper()
        for country, keywords in COUNTRY_KEYWORDS.items():
            for keyword in keywords:
                if keyword.upper() in upper_name: return country
        return "æœªçŸ¥"

    def _extract_country_from_name(self, node_name: str) -> str:
        return SubscriptionBot._extract_country_from_name_static(node_name)

    def _parse_node_lines(self, lines_to_parse: list[str], node_info: dict) -> None:
        parse_node_lines_with_b64(lines_to_parse, node_info)

    async def fetch_url_data(self, session: ClientSession, url: str, content_override: str = None):
        # --- ä¿®æ”¹å¼€å§‹ï¼šå¢åŠ å¸¦TTLçš„å†…å­˜ç¼“å­˜é€»è¾‘ ---
        # æ£€æŸ¥å†…å­˜ç¼“å­˜ï¼Œcontent_overrideä¸ä¸ºNoneæ—¶åˆ™è·³è¿‡ç¼“å­˜
        if content_override is None and url in self._node_info_cache:
            cache_time, cached_data = self._node_info_cache[url]
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦åœ¨30åˆ†é’Ÿæœ‰æ•ˆæœŸå†…
            if (time.time() - cache_time) < CACHE_TTL_SECONDS:
                return cached_data
            else:
                # ç¼“å­˜å·²è¿‡æœŸï¼Œå°†å…¶åˆ é™¤
                del self._node_info_cache[url]
        # --- ä¿®æ”¹ç»“æŸ ---

        node_info = {"node_count": 0, "countries": set(), "protocol_types": set(), "all_nodes_list": [], "all_node_links": []}
        header_info = None
        text_content_to_parse = ""

        async with self._sem:
            if content_override:
                text_content_to_parse = content_override
            elif not url.lower().startswith(('http://', 'https://')):
                text_content_to_parse = url
            else:
                for i in range(3):
                    try:
                        req_url = self._append_timestamp(url)
                        headers = {"User-Agent": random.choice(CLIENT_USER_AGENTS)}
                        response = await fetch(req_url, { headers: { "User-Agent": "Mozilla/5.0" }, method: "GET", })
                            if resp.status == 200:
                                # --- æµé‡å¤´ä¿¡æ¯è§£æ ---
                                header_info_str = resp.headers.get("subscription-userinfo")
                                if header_info_str:
                                    try:
                                        kv = dict(RE_USERINFO_KV.findall(header_info_str))
                                        header_info = {
                                            "upload": int(kv.get("upload", 0)),
                                            "download": int(kv.get("download", 0)),
                                            "total": int(kv.get("total", 0)),
                                            "expire": int(kv.get("expire", 0)) or None,
                                        }
                                        header_info["remaining_bytes"] = max(header_info["total"] - (header_info["upload"] + header_info["download"]), 0)
                                        header_info["has_exp"] = bool(header_info["expire"])
                                        header_info["remaining_secs"] = header_info["expire"] - int(time.time()) if header_info["expire"] else None
                                    except Exception:
                                        header_info = None
                                
                                # --- èŠ‚ç‚¹å†…å®¹è§£æ ---
                                raw_content = await resp.read()
                                text_content_to_parse = raw_content.decode('utf-8', 'ignore')
                                break # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    except (asyncio.TimeoutError, ClientError):
                        if i < 2: await asyncio.sleep(0.5 * (i + 1))
            
            if text_content_to_parse:
                parsed_successfully = False
                try:
                    all_docs = try_yaml_parse(text_content_to_parse)
                    for config in all_docs:
                        proxies_list = find_proxies_in_config(config)
                        if proxies_list:
                            for proxy in proxies_list:
                                node_info["node_count"] += 1
                                protocol_type = proxy.get('type', 'UNKNOWN').upper()
                                node_info["protocol_types"].add(protocol_type)
                                node_name = extract_node_name(proxy)
                                node_info["all_nodes_list"].append({"name": node_name, "protocol": protocol_type})
                                country = self._extract_country_from_name(node_name)
                                if country != "æœªçŸ¥": node_info["countries"].add(country)
                            parsed_successfully = True
                            break
                except Exception:
                    pass

                if not parsed_successfully:
                    decoded_content = safe_b64decode(text_content_to_parse)
                    lines_to_parse = decoded_content.splitlines() if decoded_content else text_content_to_parse.splitlines()
                    self._parse_node_lines(lines_to_parse, node_info)

        # å‡†å¤‡æœ€ç»ˆçš„è¿”å›ç»“æœ
        result_node_data = {
            "node_count": node_info["node_count"],
            "country_count": len(node_info["countries"]),
            "countries": sorted(list(node_info["countries"]), key=lambda x: ORDERED_COUNTRIES.index(x) if x in ORDERED_COUNTRIES else len(ORDERED_COUNTRIES)),
            "protocol_types": sorted(list(node_info["protocol_types"])),
            "all_nodes_list": node_info["all_nodes_list"],
            "all_node_links": node_info["all_node_links"]
        }
        
        # å°†èŠ‚ç‚¹å’Œæµé‡ä¿¡æ¯æ‰“åŒ…æˆä¸€ä¸ªå¯¹è±¡
        final_result = {"node_data": result_node_data, "header_info": header_info}
        
        # --- ä¿®æ”¹å¼€å§‹ï¼šå­˜å…¥ç¼“å­˜æ—¶ï¼ŒåŒæ—¶ä¿å­˜å½“å‰æ—¶é—´æˆ³ ---
        if content_override is None:
            self._node_info_cache[url] = (time.time(), final_result)
        # --- ä¿®æ”¹ç»“æŸ ---
            
        return final_result

    def _append_timestamp(self, url: str) -> str:
        ts = int(time.time() * 1000)
        return f"{url}&_={ts}" if "?" in url else f"{url}?_={ts}"


async def process_and_format_url(session: ClientSession, bot_inst: 'SubscriptionBot', url: str,
                                 original_filename: Optional[str] = None, content_override: Optional[str] = None):
    # åªè°ƒç”¨ä¸€æ¬¡ç»Ÿä¸€çš„è·å–å‡½æ•°
    full_data = await bot_inst.fetch_url_data(session, url, content_override=content_override)
    
    node_data = full_data.get("node_data", {})
    info = full_data.get("header_info") # ç›´æ¥ä»ç»“æœä¸­è·å–æµé‡ä¿¡æ¯

    node_count = node_data.get("node_count", 0)
    status = {"valid": False, "exhausted": False, "expired": False, "source_type": "url"}
    if url.startswith("file_content_direct://"): status["source_type"] = "file"
    if node_count > 0: status["valid"] = True
    else:
        return {"url": url, "summary_text": "", "detailed_text": "", "status": status, "all_nodes_list": [],
                "all_node_links": [], "original_filename": original_filename}

    name = 'æœªçŸ¥æœºåœº'
    if status["source_type"] == "file" and original_filename:
        name = original_filename.rsplit('.', 1)[0]
    else:
        name = await bot_inst.extract_name(session, url)

    country_count = node_data.get("country_count", 0)
    protocol_types = ",".join([x.lower() for x in node_data.get("protocol_types", [])]) or "æœªçŸ¥"
    countries_list = ",".join(node_data.get("countries", [])) or "æœªçŸ¥"
    safe_name, safe_url, safe_filename_str = safe_html(name), safe_html(url), safe_html(original_filename or 'ä¸Šä¼ æ–‡ä»¶')

    main_part_template = f"æœºåœºåç§°: {safe_name}\n"
    if status["source_type"] == "url":
        if url.lower().startswith(('http://', 'https://')):
            main_part_template += f'è®¢é˜…é“¾æ¥: <a href="{safe_url}">{safe_url}</a>\n'
        else:
            main_part_template = f"èŠ‚ç‚¹è¯¦æƒ…:\n"
    else:
        main_part_template += f"æ–‡ä»¶æ¥æº: {safe_filename_str}\n"

    # è¿™é‡Œçš„åˆ¤æ–­é€»è¾‘ä¸å˜ï¼Œä½† `info` çš„æ¥æºæ›´å¯é äº†
    if info and info.get('total', 0) > 0:
        used_bytes = info["upload"] + info["download"]
        used_pct = (used_bytes / info["total"] * 100) if info["total"] > 0 else 0
        remaining_bytes = info["remaining_bytes"]
        if remaining_bytes <= 1024 * 1024: status['exhausted'] = True
        exp_time = "é•¿æœŸæœ‰æ•ˆ"
        if info.get('has_exp') and info.get('expire'):
            if info['remaining_secs'] < 0:
                status['expired'], exp_time = True, "å·²è¿‡æœŸ"
            elif info['remaining_secs'] < 365 * 10 * 86400:
                expire_dt = datetime.fromtimestamp(info["expire"])
                exp_time = f"{expire_dt.strftime('%Y-%m-%d')} (å‰©{info['remaining_secs'] // 86400}å¤©)"
        main_part_dynamic = (
            f"æµé‡è¯¦æƒ…: {format_traffic(used_bytes)} / {format_traffic(info['total'])}\n"
            f"ä½¿ç”¨è¿›åº¦: {gen_bar(used_pct)} {used_pct:.1f}%\n"
            f"å‰©ä½™å¯ç”¨: {format_traffic(remaining_bytes)}\n"
            f"è¿‡æœŸæ—¶é—´: {safe_html(exp_time)}"
        )
    else:
        proxies_to_scan = node_data.get("all_nodes_list", [])
        quotas, expires = scan_proxies_quota_expire(proxies_to_scan)
        main_part_dynamic = (
            f"æµé‡è¯¦æƒ…: {safe_html(' / '.join(quotas) or 'æœªçŸ¥')}\n"
            f"è¿‡æœŸæ—¶é—´: {safe_html('ã€'.join(expires) or 'é•¿æœŸæœ‰æ•ˆ')}"
        )

    all_nodes_list = node_data.get("all_nodes_list", [])
    total_nodes = len(all_nodes_list)
    if not url.lower().startswith(('http://', 'https://')) and total_nodes == 1:
        summary_nodes_info = (f"èŠ‚ç‚¹åç§°: {safe_html(all_nodes_list[0]['name'])}\n"
                              f"åè®®ç±»å‹: {safe_html(protocol_types)}")
    else:
        summary_nodes_info = (f"åè®®ç±»å‹: {safe_html(protocol_types)}\n"
                              f"èŠ‚ç‚¹æ€»æ•°: {node_count} | å›½å®¶/åœ°åŒº: {country_count}\n"
                              f"èŠ‚ç‚¹èŒƒå›´: {safe_html(countries_list)}")
    
    nodes_text_formatted = ""
    if all_nodes_list and not (not url.lower().startswith(('http://', 'https://')) and total_nodes == 1):
        formatted_nodes = [f"- {safe_html(node['protocol'].lower())}: {safe_html(node['name'])}" for node in all_nodes_list[:NODE_DISPLAY_LIMIT]]
        nodes_text_formatted = "\n".join(formatted_nodes)
        if total_nodes > NODE_DISPLAY_LIMIT:
            nodes_text_formatted += f"\n... ç­‰ç­‰ ({total_nodes - NODE_DISPLAY_LIMIT} ä¸ªæ›´å¤šèŠ‚ç‚¹æœªæ˜¾ç¤º)"

    summary_text = main_part_template + main_part_dynamic + f"\n<pre><code>{summary_nodes_info}</code></pre>"
    detailed_text = summary_text
    if nodes_text_formatted:
        detailed_text += f"\n<pre><code>{nodes_text_formatted}</code></pre>"

    if status.get('exhausted') or status.get('expired'):
        summary_text, detailed_text = f"<del>{summary_text}</del>", f"<del>{detailed_text}</del>"
    return {"url": url, "summary_text": summary_text, "detailed_text": detailed_text, "status": status,
            "all_nodes_list": all_nodes_list, "all_node_links": node_data.get("all_node_links", []),
            "original_filename": original_filename}


async def _extract_sources_from_message(message: Message) -> tuple[list, Optional[str], Optional[str], list]:
    urls, source_filename, text_content, initial_messages = [], None, None, []
    if message.document and message.document.file_name:
        original_file_name = message.document.file_name
        file_ext = os.path.splitext(original_file_name)[1].lower()
        if file_ext in ('.txt', '.yaml', '.yml'):
            try:
                file = await message.document.get_file()
                file_text = (await file.download_as_bytearray()).decode('utf-8', 'ignore')
                lines = [line.strip() for line in file_text.splitlines() if line.strip()]
                is_profile = any((
                    bool(try_yaml_parse(file_text)),
                    len(lines) == 1 and is_base64_string(lines[0]),
                    any(RE_DIRECT_NODE_LINKS.match(line) for line in lines)
                ))
                if is_profile:
                    urls.append(f"file_content_direct://{uuid.uuid4().hex}")
                    source_filename, text_content = original_file_name, file_text
                else:
                    extracted_urls = RE_URLS.findall(file_text)
                    if extracted_urls: urls.extend(extracted_urls)
                    else:
                        urls.append(f"file_content_direct://{uuid.uuid4().hex}")
                        source_filename, text_content = original_file_name, file_text
                return urls, source_filename, text_content, initial_messages
            except Exception as e:
                initial_messages.append(f"æ— æ³•å¤„ç†æ–‡ä»¶: <code>{safe_html(original_file_name)}</code>")
    if message.text:
        found_links = RE_URLS.findall(message.text)
        if found_links:
            unique_urls = sorted(list(dict.fromkeys(found_links)))
            if len(unique_urls) > TEXT_MESSAGE_URL_LIMIT:
                initial_messages.append(f"é“¾æ¥è¿‡å¤šï¼Œä»…å¤„ç†å‰ {TEXT_MESSAGE_URL_LIMIT} æ¡ã€‚")
                urls.extend(unique_urls[:TEXT_MESSAGE_URL_LIMIT])
            else:
                urls.extend(unique_urls)
    return urls, source_filename, text_content, initial_messages


async def _process_and_get_results(update: Update, ctx: ContextTypes.DEFAULT_TYPE, urls_to_process: list[str],
                                   source_filename: Optional[str], text_content: Optional[str],
                                   processing_message: Optional[Message], initial_prefix_text: str):
    bot_inst: 'SubscriptionBot' = ctx.bot_data["bot"]
    timeout = ClientTimeout(total=30)
    results = []
    total_to_process = len(urls_to_process)
    last_update_time = time.time()

    async with ClientSession(timeout=timeout) as session:
        tasks = [asyncio.create_task(process_and_format_url(
            session, bot_inst, url,
            original_filename=source_filename,
            content_override=text_content if url.startswith("file_content_direct://") else None
        )) for url in urls_to_process]
        for i, future in enumerate(asyncio.as_completed(tasks)):
            try:
                results.append(await future)
            except Exception as e:
                print(f"å¤„ç†URLæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                results.append(None)
            
            current_time = time.time()
            if processing_message and total_to_process > 5 and (current_time - last_update_time > 0.5 or (i + 1) == total_to_process):
                try:
                    await processing_message.edit_text(initial_prefix_text + _format_progress_bar(i + 1, total_to_process), parse_mode="HTML")
                    last_update_time = current_time
                except BadRequest: pass
    return results


async def _format_and_reply(update: Update, ctx: ContextTypes.DEFAULT_TYPE, results: list,
                            urls_to_process: list, source_filename: Optional[str], text_content: Optional[str],
                            processing_message: Optional[Message]):
    if processing_message:
        try:
            await processing_message.delete()
        except BadRequest:
            pass

    valid_urls, invalid_urls, node_links = [], [], []
    for r in results:
        if isinstance(r, dict):
            status, url = r.get('status', {}), r.get('url')
            if status.get('source_type') == 'url' and url and url.lower().startswith(('http', 'https')):
                (valid_urls if status.get('valid') and not status.get('exhausted') and not status.get('expired') else invalid_urls).append(url)
            if r.get('all_node_links'): node_links.extend(r.get('all_node_links'))
    if valid_urls: update_cache_file(valid_urls)
    if invalid_urls: update_invalid_cache_file(invalid_urls)
    if node_links: update_valid_nodes_file(list(set(node_links)))

    valid_results, invalid_results = [], []
    for r in results:
        if r and r.get('status', {}).get('valid'):
            (valid_results if not r.get('status',{}).get('exhausted') and not r.get('status',{}).get('expired') else invalid_results).append(r)

    total, valid_c, invalid_c = len(urls_to_process), len(valid_results), len(invalid_results)
    exhausted_c = sum(1 for r in invalid_results if r.get('status',{}).get('exhausted'))
    expired_c = sum(1 for r in invalid_results if r.get('status',{}).get('expired'))
    failed_c = total - (valid_c + invalid_c)
    stats_line = ""
    if total > 1:
        stats_line = f"\n\næŸ¥è¯¢ç»Ÿè®¡: æœ‰æ•ˆ: {valid_c} | è€—å°½: {exhausted_c} | è¿‡æœŸ: {expired_c} | å¤±æ•ˆ: {failed_c}"

    if total > FILE_OUTPUT_THRESHOLD:
        await update.message.reply_text(f"æ£€æµ‹åˆ° {total} æ¡é“¾æ¥ï¼Œè¶…è¿‡é˜ˆå€¼ï¼Œç»“æœå°†ä»¥æ–‡ä»¶å½¢å¼å‘é€ã€‚" + stats_line, parse_mode="HTML")
        
        def clean_html(html_text: str) -> str:
            # 1. å…ˆå°† <pre><code> è¿™ç§ç‰¹æ®Šç»“æ„è½¬æ¢æˆæ¢è¡Œ
            text_with_newlines = html_text.replace('<pre><code>', '\n').replace('</code></pre>', '')
            # 2. ä½¿ç”¨é€šç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œç§»é™¤æ‰€æœ‰ <...> å½¢å¼çš„HTMLæ ‡ç­¾
            plain_text = re.sub(r'<[^>]+>', '', text_with_newlines)
            # 3. å¤„ç†åƒ &amp; è¿™æ ·çš„HTMLå®ä½“ç¼–ç ï¼Œå¹¶å»é™¤é¦–å°¾å¤šä½™çš„ç©ºç™½
            return html.unescape(plain_text).strip()

        if valid_results:
            await update.message.reply_document(document=InputFile(io.BytesIO("\n\n".join(clean_html(r['summary_text']) for r in valid_results).encode('utf-8')), filename="æœ‰æ•ˆè®¢é˜….txt"))
        if invalid_results:
            await update.message.reply_document(document=InputFile(io.BytesIO("\n\n".join(clean_html(r['summary_text']) for r in invalid_results).encode('utf-8')), filename="æ— æ•ˆè®¢é˜….txt"))
        return

    # --- ä¿®æ”¹ç‚¹ #2ï¼šè‡ªåŠ¨åˆ é™¤æ— æ³•è§£æçš„æç¤º ---
    if not valid_results and not invalid_results:
        msg_to_delete = await update.message.reply_text("äº²~ æ‚¨å‘é€çš„å†…å®¹æ— æ³•è§£æï¼Œè¯·æ£€æŸ¥åé‡è¯•ã€‚")
        await asyncio.sleep(2)
        try:
            await msg_to_delete.delete()
        except (BadRequest, TimedOut, NetworkError):
            pass
        return
    # --- ä¿®æ”¹ç‚¹ç»“æŸ ---

    message_parts = []
    if valid_results: message_parts.append("\n\n".join(r['summary_text'] for r in valid_results))
    if invalid_results: message_parts.append("\n\n".join(r['summary_text'] for r in invalid_results))
    summary_text = "\n\n".join(message_parts) + stats_line
    if failed_c > 0 and total == 1: summary_text += "\n\nâ„¹ï¸ è¯¥é“¾æ¥/æ–‡ä»¶æ— æ³•å¤„ç†æˆ–è§£æå¤±è´¥ã€‚"

    if len(summary_text) > 4096:
        await update.message.reply_text("ç»“æœå†…å®¹è¿‡é•¿ï¼Œå·²è½¬ä¸ºæ–‡ä»¶å‘é€ã€‚", document=InputFile(io.BytesIO(re.sub('<[^<]+?>', '', summary_text).encode('utf-8')), filename="æŸ¥è¯¢ç»“æœ.txt"))
    else:
        job_id = uuid.uuid4().hex
        all_results = valid_results + invalid_results
        detailed_text = "\n\n".join(r['detailed_text'] for r in all_results) + stats_line
        ctx.bot_data["refresh_jobs"][job_id] = {
            "tasks": [{'type': 'file' if r.get('status',{}).get('source_type') == 'file' else 'url', 'url': r['url'], 'filename': r.get('original_filename'), 'content': text_content if r.get('status',{}).get('source_type') == 'file' else None} for r in all_results],
            "summary_view_text": summary_text, "detailed_view_text": detailed_text, "current_view_is_detailed": False
        }
        keyboard = [[InlineKeyboardButton("åˆ·æ–°", callback_data=f"refresh:{job_id}"), InlineKeyboardButton("æ˜¾ç¤ºèŠ‚ç‚¹", callback_data=f"show_nodes:{job_id}")]]
        await update.message.reply_text(summary_text, parse_mode="HTML", reply_markup=InlineKeyboardMarkup(keyboard), disable_web_page_preview=True)


async def handle_message(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    message = update.message
    if not message: return
    urls, source_filename, text_content, initial_messages = await _extract_sources_from_message(message)
    all_urls = sorted(list(dict.fromkeys(urls)))
    if not all_urls: return
    
    if len(all_urls) > QUERY_URL_LIMIT:
        initial_messages.append(f"æ€»é“¾æ¥æ•°è¶…è¿‡ {QUERY_URL_LIMIT}ï¼Œå·²æˆªæ–­ã€‚")
        all_urls = all_urls[:QUERY_URL_LIMIT]

    # --- ä¿®æ”¹ç‚¹ #1ï¼šä¼˜åŒ–æç¤ºé€»è¾‘ ---
    num_to_process = len(all_urls)
    initial_prefix = "\n".join(initial_messages)
    
    status_text = f"æ£€æµ‹åˆ° {num_to_process} ä¸ªé“¾æ¥/æ–‡ä»¶ï¼Œæ­£åœ¨ä¸ºæ‚¨æŸ¥è¯¢â€¦â€¦"
    full_initial_text = (initial_prefix + "\n\n" + status_text) if initial_prefix else status_text
    processing_msg = await message.reply_text(full_initial_text, parse_mode="HTML")

    initial_prefix_for_progress = (initial_prefix + "\n\n" if initial_prefix else "")
    if num_to_process > 5:
        progress_bar_text = _format_progress_bar(0, num_to_process)
        try:
            await processing_msg.edit_text(initial_prefix_for_progress + progress_bar_text, parse_mode="HTML")
        except BadRequest:
            pass
    # --- ä¿®æ”¹ç‚¹ç»“æŸ ---

    results = await _process_and_get_results(update, ctx, all_urls, source_filename, text_content, processing_msg, initial_prefix_for_progress)
    
    await _format_and_reply(update, ctx, results, all_urls, source_filename, text_content, processing_msg)
    
    if NOTIFICATION_ID:
        if any(r and r.get('status', {}).get('valid') and not r.get('status', {}).get('exhausted') and not r.get('status', {}).get('expired') for r in results):
            await send_notification_to_admin(ctx.bot, update, results)


async def refresh_button_callback(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    bot_inst: SubscriptionBot = ctx.bot_data["bot"]
    try:
        job_id = query.data.split(":", 1)[1]
    except (IndexError, AttributeError):
        await query.edit_message_text("é”™è¯¯ï¼šæ— æ•ˆçš„å›è°ƒæ•°æ®ã€‚", reply_markup=None)
        return
    job_data = ctx.bot_data.get("refresh_jobs", {}).get(job_id)
    if not job_data or "tasks" not in job_data:
        # --- ä¿®æ”¹å¼€å§‹ ---
        msg_to_delete = await query.edit_message_text("æŠ±æ­‰ï¼Œæ­¤åˆ·æ–°è¯·æ±‚å·²è¿‡æœŸã€‚", reply_markup=None)
        await asyncio.sleep(2)
        try:
            await msg_to_delete.delete()
        except (BadRequest, TimedOut, NetworkError):
            pass
        return
        # --- ä¿®æ”¹ç»“æŸ ---
        
    for task in job_data["tasks"]:
        url = task['url']
        # åˆ·æ–°æ—¶ï¼Œä¸»åŠ¨ä»å†…å­˜ç¼“å­˜ä¸­ç§»é™¤
        bot_inst._node_info_cache.pop(url, None)
        bot_inst._name_cache.pop(url, None)
        
    await query.answer(text="æ­£åœ¨åˆ·æ–°ï¼Œè¯·ç¨å€™...")
    
    timeout = ClientTimeout(total=30)
    async with ClientSession(timeout=timeout) as session:
        tasks = [asyncio.create_task(process_and_format_url(session, bot_inst, task['url'], original_filename=task.get('filename'), content_override=task.get('content'))) for task in job_data["tasks"]]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
    valid_urls, invalid_urls, node_links = [], [], []
    for r in results:
        if isinstance(r, dict):
            status, url = r.get('status', {}), r.get('url')
            if status.get('source_type') == 'url' and url and url.lower().startswith(('http', 'https')):
                (valid_urls if status.get('valid') and not status.get('exhausted') and not status.get('expired') else invalid_urls).append(url)
            if r.get('all_node_links'): node_links.extend(r.get('all_node_links'))
    if valid_urls: update_cache_file(valid_urls)
    if invalid_urls: update_invalid_cache_file(invalid_urls)
    if node_links: update_valid_nodes_file(list(set(node_links)))

    valid_results, invalid_results = [], []
    for r in results:
        if r and isinstance(r, dict) and r.get('status', {}).get('valid'):
            (valid_results if not r.get('status',{}).get('exhausted') and not r.get('status',{}).get('expired') else invalid_results).append(r)

    total = len(job_data["tasks"])
    stats_line = ""
    if total > 1:
        stats_line = f"\n\næŸ¥è¯¢ç»Ÿè®¡: æœ‰æ•ˆ: {len(valid_results)} | è€—å°½: {sum(1 for r in invalid_results if r.get('status',{}).get('exhausted'))} | è¿‡æœŸ: {sum(1 for r in invalid_results if r.get('status',{}).get('expired'))} | å¤±æ•ˆ: {total - (len(valid_results) + len(invalid_results))}"

    message_parts = []
    if valid_results: message_parts.append("\n\n".join(r['summary_text'] for r in valid_results))
    if invalid_results: message_parts.append("\n\n".join(r['summary_text'] for r in invalid_results))
    summary_text = "\n\n".join(message_parts) + stats_line
    
    all_results = valid_results + invalid_results
    detailed_text = "\n\n".join(r['detailed_text'] for r in all_results) + stats_line
    
    job_data["summary_view_text"], job_data["detailed_view_text"] = summary_text, detailed_text
    
    new_text = detailed_text if job_data["current_view_is_detailed"] else summary_text
    button_text = "æŠ˜å èŠ‚ç‚¹" if job_data["current_view_is_detailed"] else "æ˜¾ç¤ºèŠ‚ç‚¹"
    callback_action = "hide_nodes" if job_data["current_view_is_detailed"] else "show_nodes"
    
    if not new_text.strip(): new_text = "åˆ·æ–°åæœªæ‰¾åˆ°æœ‰æ•ˆä¿¡æ¯ã€‚"
    if len(new_text) > 4096:
        await query.answer(text="åˆ·æ–°åå†…å®¹è¿‡é•¿ï¼Œæ— æ³•æ˜¾ç¤ºã€‚", show_alert=True)
        return
        
    keyboard = [[InlineKeyboardButton("åˆ·æ–°", callback_data=f"refresh:{job_id}"), InlineKeyboardButton(button_text, callback_data=f"{callback_action}:{job_id}")]]
    try:
        await query.edit_message_text(text=new_text, parse_mode="HTML", reply_markup=InlineKeyboardMarkup(keyboard), disable_web_page_preview=True)
    except BadRequest as e:
        if 'Message is not modified' not in str(e): print(f"åˆ·æ–°æ—¶ç¼–è¾‘æ¶ˆæ¯å¤±è´¥: {e}")
        else: await query.answer(text="ä¿¡æ¯æ— å˜åŒ–ã€‚")


async def toggle_nodes_view_callback(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    try:
        action, job_id = query.data.split(":", 1)
    except (ValueError, IndexError):
        await query.edit_message_text("é”™è¯¯ï¼šæ— æ•ˆçš„å›è°ƒæ•°æ®ã€‚", reply_markup=None)
        return
    job_data = ctx.bot_data.get("refresh_jobs", {}).get(job_id)
    if not job_data:
        await query.edit_message_text("æŠ±æ­‰ï¼Œæ­¤è¯·æ±‚å·²è¿‡æœŸã€‚", reply_markup=None)
        return
        
    job_data["current_view_is_detailed"] = (action == "show_nodes")
    new_text = job_data["detailed_view_text"] if job_data["current_view_is_detailed"] else job_data["summary_view_text"]
    button_text = "æŠ˜å èŠ‚ç‚¹" if job_data["current_view_is_detailed"] else "æ˜¾ç¤ºèŠ‚ç‚¹"
    callback_action = "hide_nodes" if job_data["current_view_is_detailed"] else "show_nodes"

    if len(new_text) > 4096:
        await query.answer(text="å†…å®¹è¿‡é•¿ï¼Œæ— æ³•å±•å¼€/æŠ˜å ã€‚", show_alert=True)
        return
        
    await query.answer()
    keyboard = [[InlineKeyboardButton("åˆ·æ–°", callback_data=f"refresh:{job_id}"), InlineKeyboardButton(button_text, callback_data=f"{callback_action}:{job_id}")]]
    try:
        await query.edit_message_text(text=new_text, parse_mode="HTML", reply_markup=InlineKeyboardMarkup(keyboard), disable_web_page_preview=True)
    except BadRequest as e:
        if 'Message is not modified' not in str(e): print(f"åˆ‡æ¢è§†å›¾æ—¶ç¼–è¾‘æ¶ˆæ¯å¤±è´¥: {e}")


async def start(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("è®¢é˜…ä¿¡æ¯æ£€æŸ¥æœºå™¨äººå·²å¯åŠ¨ï¼\nå‘é€è®¢é˜…é“¾æ¥ã€èŠ‚ç‚¹é“¾æ¥æˆ–åŒ…å«è¿™äº›å†…å®¹çš„ .txt/.yaml/.yml æ–‡ä»¶å³å¯æŸ¥è¯¢ã€‚")


async def help_command(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    help_text = (
        "ä½¿ç”¨è¯´æ˜\n\n"
        "å¦‚ä½•æŸ¥è¯¢:\n"
        "ç›´æ¥å‘æœºå™¨äººå‘é€ä»¥ä¸‹ä»»æ„å†…å®¹ï¼š\n"
        "  â€¢ è®¢é˜…é“¾æ¥ (http://...)\n"
        "  â€¢ åŒ…å«è®¢é˜…é“¾æ¥æˆ–èŠ‚ç‚¹é“¾æ¥çš„ <code>.txt</code> æ–‡ä»¶\n"
        "  â€¢ Clash é…ç½®æ–‡ä»¶ (<code>.yaml</code> / <code>.yml</code>)\n\n"
        "ç»“æœäº¤äº’:\n"
        "  â€¢ åˆ·æ–°: é‡æ–°è·å–å¹¶æ›´æ–°å½“å‰è®¢é˜…çš„ä¿¡æ¯ã€‚\n"
        "  â€¢ æ˜¾ç¤º/æŠ˜å èŠ‚ç‚¹: å±•å¼€æˆ–æ”¶èµ·è¯¦ç»†çš„èŠ‚ç‚¹åˆ—è¡¨ã€‚\n\n"
        "ç®¡ç†å‘˜æŒ‡ä»¤:\n"
        "<code>/zj domain=åç§°</code> - æ·»åŠ åŸŸåæ˜ å°„\n"
        "<code>/zj</code> - æŸ¥çœ‹æ‰€æœ‰æ˜ å°„\n"
        "<code>/clear</code> - æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"
    )
    await update.message.reply_text(help_text, parse_mode="HTML")


async def ping_command(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    latency = int((time.time() - update.message.date.timestamp()) * 1000)
    await update.message.reply_text(f"Bot åœ¨çº¿ï¼Œå»¶è¿Ÿçº¦ {latency} ms")


async def zj_command(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    if update.effective_user.id not in ADMIN_IDS: return
    bot_inst: SubscriptionBot = ctx.bot_data["bot"]
    arg = update.message.text.partition(" ")[2].strip()
    if not arg:
        if not bot_inst.domain_map: return await update.message.reply_text("å½“å‰æ²¡æœ‰åŸŸåæ˜ å°„ã€‚")
        map_str = "\n".join(f"<code>{safe_html(d)}</code> â†’ <code>{safe_html(n)}</code>" for d, n in bot_inst.domain_map.items())
        return await update.message.reply_text(f"å½“å‰åŸŸåæ˜ å°„ï¼š\n{map_str}", parse_mode="HTML")
    if "=" not in arg: return await update.message.reply_text("ç”¨æ³•ï¼š<code>/zj domain=åç§°</code>", parse_mode="HTML")
    d, n = (s.strip() for s in arg.split("=", 1))
    bot_inst.domain_map[d] = n
    bot_inst.save_domain_map()
    bot_inst._name_cache.clear()
    await update.message.reply_text(f"å·²æ·»åŠ æ˜ å°„ï¼š<code>{safe_html(d)}</code> â†’ <code>{safe_html(n)}</code>", parse_mode="HTML")


async def zh_command(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    if update.effective_user.id not in ADMIN_IDS: return
    sent_count = 0
    for caption, file_path in {"æœ‰æ•ˆè®¢é˜…": CACHE_FILE, "æ— æ•ˆè®¢é˜…": INVALID_CACHE_FILE, "æœ‰æ•ˆèŠ‚ç‚¹": VALID_NODES_FILE}.items():
        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
            try:
                await ctx.bot.send_document(chat_id=update.effective_chat.id, document=open(file_path, "rb"), filename=os.path.basename(file_path), caption=f"ç¼“å­˜çš„{caption}æ–‡ä»¶ã€‚")
                sent_count += 1
            except Exception as e:
                await update.message.reply_text(f"å‘é€æ–‡ä»¶ {file_path} å‡ºé”™: {e}")
    if sent_count == 0: await update.message.reply_text("å½“å‰æ²¡æœ‰ä»»ä½•å¯å¯¼å‡ºçš„ç¼“å­˜æ–‡ä»¶ã€‚")


async def clear_command(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    if update.effective_user.id not in ADMIN_IDS: return
    msg = await update.message.reply_text("æ­£åœ¨æ¸…ç†ç¼“å­˜...")
    # æ¸…ç†æ–‡ä»¶ç¼“å­˜
    for file_path in [CACHE_FILE, INVALID_CACHE_FILE, VALID_NODES_FILE]:
        if os.path.exists(file_path): os.remove(file_path)
        open(file_path, 'w').close()
    if os.path.exists(TEMP_YAML_DIR): shutil.rmtree(TEMP_YAML_DIR)
    os.makedirs(TEMP_YAML_DIR, exist_ok=True)
    
    # æ¸…ç†å†…å­˜ç¼“å­˜
    bot_inst: SubscriptionBot = ctx.bot_data["bot"]
    bot_inst._name_cache.clear()
    bot_inst._node_info_cache.clear()
    ctx.bot_data["refresh_jobs"].clear()
    
    await msg.edit_text("æ¸…ç†å®Œæˆã€‚")


async def set_bot_commands(app):
    await app.bot.set_my_commands([
        BotCommand("start", "å¯åŠ¨æœºå™¨äºº"),
        BotCommand("help", "è·å–å¸®åŠ©è¯´æ˜"),
        BotCommand("ping", "æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ"),
        BotCommand("zj", "è®¾ç½®åŸŸåæ˜ å°„ (ç®¡ç†å‘˜)"),
        BotCommand("clear", "æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ (ç®¡ç†å‘˜)"),
    ])


def main():
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    if not token:
        print("é”™è¯¯: ç¯å¢ƒå˜é‡ TELEGRAM_BOT_TOKEN æœªè®¾ç½®ã€‚")
        return

    os.makedirs(TEMP_YAML_DIR, exist_ok=True)
    for f in [CACHE_FILE, INVALID_CACHE_FILE, VALID_NODES_FILE, DOMAIN_MAP_FILE]:
        if not os.path.exists(f):
            try:
                with open(f, 'w', encoding='utf-8') as fp: pass
            except Exception as e:
                print(f"åˆ›å»ºæ–‡ä»¶ {f} å¤±è´¥: {e}")

    app = Application.builder().token(token).build()
    app.bot_data["bot"] = SubscriptionBot()
    app.bot_data["refresh_jobs"] = {}

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(CommandHandler("ping", ping_command))
    app.add_handler(CommandHandler("zj", zj_command))
    app.add_handler(CommandHandler("zh", zh_command))
    app.add_handler(CommandHandler("clear", clear_command))
    app.add_handler(MessageHandler(filters.TEXT | filters.Document.ALL, handle_message))
    app.add_handler(CallbackQueryHandler(refresh_button_callback, pattern=r"^refresh:"))
    app.add_handler(CallbackQueryHandler(toggle_nodes_view_callback, pattern=r"^(show_nodes|hide_nodes):"))

    app.post_init = set_bot_commands
    print("Bot is running...")
    app.run_polling()


if __name__ == "__main__":
    main()